{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN_CharacterModel.ipynb","provenance":[],"authorship_tag":"ABX9TyNkBQtQIN2vcdQcTTwhaBkt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"GyKkuiN1duFV","executionInfo":{"status":"ok","timestamp":1605949315138,"user_tz":-330,"elapsed":1303,"user":{"displayName":"Partha Sarathi Mukherjee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdDTlcE87KO7N4v7Uo84qM1TnzxH78UhyugRxz=s64","userId":"13597278281136863767"}}},"source":["import tensorflow as tf\n","import numpy as np\n","import sys"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WNyEL812d2na"},"source":["Time to load data\n","\n","\n","\n","1.   Load all data\n","2.   Make One Hot Encoding\n","3.   Prepare input and output sequence\n","\n","a b i l i t y\n","\n","a -> b\n","\n","b -> i, a,b -> i\n","\n","i -> l, a,b,i -> l\n","\n","l -> i, a,b,i,l -> i\n","\n","i -> t, a,b,i,l,i -> t\n","\n","t -> y, a,b,i,l,i,t -> y\n","\n","input sequence **a b i l i t**\n","\n","output sequence **b i l i t y**\n","\n"]},{"cell_type":"code","metadata":{"id":"mzNBwX6_d629","executionInfo":{"status":"ok","timestamp":1605949318016,"user_tz":-330,"elapsed":1239,"user":{"displayName":"Partha Sarathi Mukherjee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdDTlcE87KO7N4v7Uo84qM1TnzxH78UhyugRxz=s64","userId":"13597278281136863767"}}},"source":["def prepare_data(word_filename,alphabet_filename):\n","    f = open(word_filename)\n","    line = f.readline()\n","    all_words = []\n","    while line:\n","        word = line.strip(\"\\n\")\n","        if(len(word)>2):# words with 3 or more characters\n","            all_words.append(word)\n","        line = f.readline()\n","    f.close()\n","    # sorted_words = sorted(all_words,key=lambda x:len(x))\n","    # np.random.shuffle(all_words)\n","    f = open(alphabet_filename)\n","    line = f.readline()\n","    alphabets = []\n","    while line:\n","        character = line.strip(\"\\n\")\n","        alphabets.append(character)\n","        line = f.readline()\n","    f.close()\n","    return all_words,alphabets\n","\n","def convert_to_onehot(character,all_characters):\n","    vector = np.zeros([len(all_characters)])\n","    idx = all_characters.index(character)\n","    vector[idx] = 1\n","    return vector\n","\n","def make_train_and_target(word,all_characters):\n","    word = word.lower().replace(\"'\",\"\")\n","    x = []\n","    y = []\n","    for i in range(len(word)-1):\n","        ch = word[i]\n","        x.append(convert_to_onehot(ch,all_characters))\n","        next_ch = word[i+1]\n","        y.append(convert_to_onehot(next_ch,all_characters))\n","    return x,y"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNocDA4EeMgh"},"source":["Test Data loading"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oTVGjAkteOq5","executionInfo":{"status":"ok","timestamp":1605949333977,"user_tz":-330,"elapsed":1228,"user":{"displayName":"Partha Sarathi Mukherjee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdDTlcE87KO7N4v7Uo84qM1TnzxH78UhyugRxz=s64","userId":"13597278281136863767"}},"outputId":"3af5827a-0218-467f-9221-41bfb2e544f0"},"source":["words,characters = prepare_data('/content/sample_data/words.txt','/content/sample_data/alphabates.txt')\n","print(words[:10])\n","print(characters)\n","x,y = make_train_and_target(words[100],characters)\n","print(words[100])\n","print(x)\n","print(y)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["['ability', 'able', 'about', 'above', 'accept', 'according', 'account', 'across', 'act', 'action']\n","['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","blue\n","[array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 1., 0., 0., 0., 0., 0.])]\n","[array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 1., 0., 0., 0., 0., 0.]), array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0.])]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hhYJzomOeWv7"},"source":["Create RNN Encoder with three hardcoded layers"]},{"cell_type":"code","metadata":{"id":"TonBK4dveVzN","executionInfo":{"status":"ok","timestamp":1605950772417,"user_tz":-330,"elapsed":1232,"user":{"displayName":"Partha Sarathi Mukherjee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdDTlcE87KO7N4v7Uo84qM1TnzxH78UhyugRxz=s64","userId":"13597278281136863767"}}},"source":["class MYRNN(tf.keras.Model):\n","  def __init__(self,Nc):\n","    super(MYRNN,self).__init__()\n","    # Gated Recurrent Unit, extension of regular RNN, works like LSTM\n","    self.rnn_1 = tf.keras.layers.GRU(16,return_sequences=True) # return sequences returns all outputs from timesteps\n","    self.rnn_2 = tf.keras.layers.GRU(32,return_sequences=True)\n","    self.rnn_3 = tf.keras.layers.GRU(64,return_sequences=True) # set resturn_sequence = False for M:1 model\n","    self.output_layer = tf.keras.layers.Dense(Nc,activation='softmax') # Time distributed Dense\n","    print(\"Initalized\")\n","  \n","  def call(self,inputs):# inputs 3D array N,T,F\n","    rnn_1_out = self.rnn_1(inputs) # N,T,16\n","    rnn_2_out = self.rnn_2(rnn_1_out) # N,T,32\n","    rnn_3_out = self.rnn_3(rnn_2_out) # N,T,64\n","    output = self.output_layer(rnn_3_out) # N,T,Nc - in our example Nc = 26\n","    return output"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5t-n7LCejuI"},"source":["Use RNN in models, optimize for next character prediction"]},{"cell_type":"code","metadata":{"id":"iuU5qPbWerf4","executionInfo":{"status":"ok","timestamp":1605951155869,"user_tz":-330,"elapsed":1242,"user":{"displayName":"Partha Sarathi Mukherjee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdDTlcE87KO7N4v7Uo84qM1TnzxH78UhyugRxz=s64","userId":"13597278281136863767"}}},"source":["class CharacterModel:\n","  def __init__(self,Nc,save_dir):\n","    self.network = MYRNN(Nc)\n","    self.optimizer = tf.keras.optimizers.Adam(0.0001)\n","    self.checkpoint = tf.train.Checkpoint(model=self.network)\n","    self.savepath = save_dir\n","    self.weight_manager = tf.train.CheckpointManager(self.checkpoint,self.savepath,max_to_keep=1)\n","  \n","  def train_step(self,x,y):\n","    x = tf.expand_dims(x,0) # 1,T,Nc\n","    y = tf.expand_dims(y,0) # 1,T,Nc\n","    with tf.GradientTape() as tape:\n","      logit = self.network(x)\n","      loss = tf.reduce_mean(tf.keras.losses.categorical_crossentropy(y,logit,from_logits=False)) # do not apply softmax anymore\n","      weights = self.network.trainable_variables\n","      grads = tape.gradient(loss,weights) # auto grad is used\n","    self.optimizer.apply_gradients(zip(grads,weights)) # minimize the loss\n","    return loss\n","  \n","  def train(self,train_data,all_characters,epochs):\n","    total = len(train_data)\n","    for e in range(epochs): # run optimization for iterations\n","      e_loss = 0\n","      np.random.shuffle(train_data) # shuffling the words\n","      for i in range(total):\n","        x,y =  make_train_and_target(train_data[i],all_characters)\n","        b_loss = self.train_step(x,y)\n","        e_loss += b_loss\n","        sys.stdout.write(\"\\rBatch %d/%d Loss %0.4f word %s\"%(i,total,b_loss,train_data[i]))\n","        sys.stdout.flush()\n","      e_loss /= total # average loss over 1 epoch for all batches\n","      self.weight_manager.save()\n","      print(\"\\n Epoch %d/%d Loss %0.4f\"%(e,epochs,e_loss))\n","  \n","  def load_model(self):\n","    restore_from = tf.train.latest_checkpoint(self.savepath)\n","    self.checkpoint.restore(restore_from)\n","    print(\"model restored from %s\"%restore_from)\n","  \n","  def predict(self,x):\n","    x = tf.expand_dims(x,0)\n","    logit = self.network(x)\n","    prediction = tf.argmax(logit,-1)\n","    return prediction.numpy()"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2JAG8rnKewo-"},"source":["Time to train model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KLxuhbvheyn4","executionInfo":{"status":"ok","timestamp":1605951945813,"user_tz":-330,"elapsed":755302,"user":{"displayName":"Partha Sarathi Mukherjee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgdDTlcE87KO7N4v7Uo84qM1TnzxH78UhyugRxz=s64","userId":"13597278281136863767"}},"outputId":"85c51c29-be55-49f4-985e-1dcc7cd18749"},"source":["SAVE_IN = 'Weights/'\n","model = CharacterModel(len(characters),'')\n","model.train(words,characters,20)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Initalized\n","WARNING:tensorflow:Layer myrnn_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n","\n","If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n","\n","To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n","\n","Batch 971/972 Loss 3.5695 word though\n"," Epoch 0/20 Loss 3.0772\n","Batch 971/972 Loss 2.6072 word rise\n"," Epoch 1/20 Loss 2.9012\n","Batch 971/972 Loss 2.5007 word none\n"," Epoch 2/20 Loss 2.8675\n","Batch 971/972 Loss 2.9112 word argue\n"," Epoch 3/20 Loss 2.8445\n","Batch 971/972 Loss 2.9157 word account\n"," Epoch 4/20 Loss 2.8196\n","Batch 971/972 Loss 2.6047 word sell\n"," Epoch 5/20 Loss 2.7810\n","Batch 971/972 Loss 2.4604 word safe\n"," Epoch 6/20 Loss 2.7227\n","Batch 971/972 Loss 2.7657 word administration\n"," Epoch 7/20 Loss 2.6733\n","Batch 971/972 Loss 3.6018 word staff\n"," Epoch 8/20 Loss 2.6333\n","Batch 971/972 Loss 3.0845 word floor\n"," Epoch 9/20 Loss 2.5997\n","Batch 971/972 Loss 2.7986 word serious\n"," Epoch 10/20 Loss 2.5704\n","Batch 971/972 Loss 2.9011 word suffer\n"," Epoch 11/20 Loss 2.5466\n","Batch 971/972 Loss 2.8006 word offer\n"," Epoch 12/20 Loss 2.5251\n","Batch 971/972 Loss 1.6488 word war\n"," Epoch 13/20 Loss 2.5081\n","Batch 971/972 Loss 2.2107 word mouth\n"," Epoch 14/20 Loss 2.4916\n","Batch 971/972 Loss 2.6376 word media\n"," Epoch 15/20 Loss 2.4782\n","Batch 971/972 Loss 2.5296 word talk\n"," Epoch 16/20 Loss 2.4654\n","Batch 971/972 Loss 2.4457 word must\n"," Epoch 17/20 Loss 2.4543\n","Batch 971/972 Loss 1.8142 word moment\n"," Epoch 18/20 Loss 2.4436\n","Batch 971/972 Loss 3.0371 word down\n"," Epoch 19/20 Loss 2.4325\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5MYHWI-ohEt_"},"source":["**Excercise**\n","\n","\n","\n","1.   Implement Batch Mechanism with BS > 2\n","2.   Change Architecture\n","3.   Extend code that can take more than one word (include space as character)\n","\n"]}]}